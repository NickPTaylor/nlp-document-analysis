[
["index.html", "Applied NLP for Document Analysis Preface", " Applied NLP for Document Analysis Nick P Taylor 2020-06-08 Preface This is a collection of reports describing the use of NLP (Natural Language Programming) for the analysis of documents. The documents have been compiled using bookdown (Xie 2020 , 2016) and also use the tidyverse packages (Wickham 2019b; Wickham et al. 2019) extensively. References "],
["document-comparison-with-nlp.html", "Chapter 1 Document Comparison with NLP 1.1 Introduction 1.2 Read Document 1.3 Tidy Corpus 1.4 Analysis 1.5 Exploratory Analysis", " Chapter 1 Document Comparison with NLP 1.1 Introduction Herein, an example of applying NLP to legal proceedings will be demonstrated. Legal documents are notoriously verbose and the objective will be to: Summarise, compare and contrast transcriptions of cases heard on the subject of ‘Trust and Probate’ in the High Court of Justice, London. The transcriptions are publicly available via the Incorporated Council of Law Reporting in the form of PDF (Portable Document Format) documents. 1.2 Read Document The documents for analysis are described using JSON (JavaScript Object Notation) as follows: [ { &quot;name&quot;: &quot;Rittson-Thomas v Oxfordshire County Council&quot;, &quot;url&quot;: &quot;https://www.iclr.co.uk/document/2018000552/transcriptXml_2018000552_2019031210462874/pdf/download&quot;, &quot;use_fonts&quot;: [10, 13], &quot;page_range&quot;: [2, &quot;Inf&quot;] }, { &quot;name&quot;: &quot;Kingsley v Kingsley&quot;, &quot;url&quot;: &quot;https://www.iclr.co.uk/document/2019001209/transcriptXml_2019001209_2019092714065099/pdf/download&quot;, &quot;use_fonts&quot;: [10, 13], &quot;page_range&quot;: [2, &quot;Inf&quot;] }, { &quot;name&quot;: &quot;Vodafone Ltd v Office of Communications&quot;, &quot;url&quot;: &quot;https://www.iclr.co.uk/document/2019001384/transcriptXml_2019001384_2019092714070160/pdf/download&quot;, &quot;use_fonts&quot;: [10, 13], &quot;page_range&quot;: [3, &quot;Inf&quot;] } ] The fields in the JSON are: name: The name of the case url: The URL of the PDF document use_fonts: The font heights of the text for processing page_range: The range of pages to process Due to the nature of PDF documents, some preliminary inspection is required to determine the fonts size for the use_fonts field i.e. the text that contains the informative content rather than headers/footers, title, etc. This inspection was performed using the pdf_data function from pdftools package (Ooms 2020). See 3 for more detail. A utility function (extract_text) is defined which takes the fields from the JSON objects as arguments and performs the necessary operations to download the PDF documents and extract the required text. The utility function returns a data.frame with 2 columns: doc_id: name of the document. text: text for analysis. The function utility function to each document using the purrr::pmap_df function (Henry and Wickham 2020) resulting in a data frame with \\(N\\) rows where \\(N\\) is the number of documents. For NLP, the tm package is used (Feinerer and Hornik 2019 ; Feinerer, Hornik, and Meyer 2008). The first stage of analysis is to source the documents and create a corpus object; this is then inspected as a sanity check. # Utility function for reading text specified by JSON. extract_text &lt;- function(name, url, use_fonts = NULL, page_range = c(-Inf, Inf)) { # Download files to temporary location. destfile = tempfile(name, fileext = &#39;.pdf&#39;) download.file(url = url, destfile = destfile, quiet = TRUE) # Filter pages/fonts of interest. text &lt;- pdftools::pdf_data(destfile) %&gt;% dplyr::bind_rows(.id = &#39;page&#39;) %&gt;% dplyr::mutate(page = as.integer(page)) %&gt;% dplyr::filter(between(page, page_range[1], page_range[2])) if (!is.null(use_fonts)) { text &lt;- text %&gt;% dplyr::filter(height %in% use_fonts) } # Remove temporary files. unlink(destfile) # Return data frame of text. text &lt;- text %&gt;% dplyr::pull(text) %&gt;% stringr::str_c(collapse = &quot; &quot;, sep = &quot; &quot;) data.frame(doc_id = name, text = text) } # Parse JSON text/file. lst_spec &lt;- jsonlite::fromJSON(json_spec) # Read document texts to data frame. df_text &lt;- purrr::pmap_df(lst_spec, extract_text) # Generate corpus. corpus &lt;- tm::DataframeSource(df_text) %&gt;% tm::VCorpus() # Inspect corpus. corpus %&gt;% purrr::map_chr(as.character) %&gt;% purrr::map_chr(str_trunc, 500, &#39;center&#39;) %&gt;% purrr::map_chr(str_wrap, 60) %&gt;% cat(sep = &#39;\\n--------\\n&#39;) ## Lord Justice Patten : 1. The School Sites Act 1841 (“the ## 1841 Act”) was passed in order to encourage and facilitate ## the provision of up to one acre of land for use as “a site ## for a school for the education of poor persons, or for the ## residence of th...ayground or for meals. But, in this case, ## the old site remained vacant with no further possible use ## for educational purposes. 24. I would therefore allow the ## appeal. Lord Justice Hamblen : 25. I agree. Lady Justice ## Nicola Davies : 26. I also agree. ## -------- ## Lance Ashworth QC: Introduction 1. Because this case ## involves members of the same family, many of them share a ## surname. For the sake of clarity, I shall therefore, without ## intending any disrespect, refer to members of the Kingsley ## family by their gi...unable to agree this. At that hearing ## the parties may make further submissions as to any other ## matters arising out of the accounts for the Partnership as a ## consequence of this judgment and such other matters (if any) ## as cannot be agreed. 1 May 2019 ## -------- ## ADRIAN BELTRAMI QC: Introduction 1. This is the trial of ## four actions brought under CPR Part 8. The Claimant in ## each action is a Mobile Network Operator and I refer to ## them collectively as the MNOs and individually as Vodafone, ## Telefonica (or O2), H... it helpful, and indeed I think ## it positively unhelpful, for me to express views on these ## issues against a backdrop of conclusions that I have not ## reached. Determination 111. I give Judgment for the MNOs in ## the net sum claimed by each respectively. 1.3 Tidy Corpus The following transformations are applied to the corpus: Remove punctuation. Remove ‘stop’ words i.e. articles, pronouns and other common words conveying no information. Remove numbers. Reduce words to ‘stems’. The last operation, reducing words to ‘stems’, is worthy of some explanation. Consider the words: ‘regulate’, ‘regulation’, ‘regulatory’. The word stem is ‘regulat’ i.e. the longest chain of common letters. It is often appropriate to aggregate words using their stems for analysis, since the full words are likely to covey similar information in the context of the document. After the transformations have been applied, the corpus is again inspected as a sanity check. # Pre-processing steps. tidy_corpus &lt;- corpus %&gt;% tm::tm_map(tm::stripWhitespace) %&gt;% tm::tm_map(tm::content_transformer(str_to_lower)) %&gt;% tm::tm_map(tm::removePunctuation, ucp = TRUE) %&gt;% tm::tm_map(tm::removeWords, c(tm::stopwords(&#39;english&#39;), &#39;x&#39;, &#39;figure&#39;)) %&gt;% tm::tm_map(tm::removeNumbers) # Before stemming, take a copy of the original corpus to un-stem words later. before_stem_corpus &lt;- tidy_corpus tidy_corpus %&lt;&gt;% tm::tm_map(stemDocument) # Inspect tidy corpus. tidy_corpus %&gt;% purrr::map_chr(as.character) %&gt;% purrr::map_chr(str_trunc, 500, &#39;center&#39;) %&gt;% purrr::map_chr(str_wrap, 60) %&gt;% cat(sep = &#39;\\n--------\\n&#39;) ## lord justic patten school site act act pass order encourag ## facilit provis one acr land use site school educ poor person ## resid schoolmast schoolmistress otherwis purpos educ poor ## person religi use knowledg major case use provid land local ## church engl...e school use educ purpos requir activ use ## land educ children accept includ ancillari activ use site ## playground meal case old site remain vacant possibl use educ ## purpos therefor allow appeal lord justic hamblen agre ladi ## justic nicola davi also agre ## -------- ## lanc ashworth qc introduct case involv member famili mani ## share surnam sake clariti shall therefor without intend ## disrespect refer member kingsley famili given name claim ## brought follow death june roger kingsley roger led automat ## dissolut farm partn...er agreement salli occupi peascroft ## andor peascroft matter conclus will list matter consequenti ## hear counsel can address form order follow parti unabl agre ## hear parti may make submiss matter aris account partnership ## consequ judgment matter agre may ## -------- ## adrian beltrami qc introduct trial four action brought cpr ## part claimant action mobil network oper refer collect mnos ## individu vodafon telefonica o hutchison three ee materi ## distinct claim action save respect individu quantum relev ## mno defend case o...di event principl ladi kid appli event ## court appeal found breach eu law found mnos claim succeed ## none point aris determin consid help inde think posit unhelp ## express view issu backdrop conclus reach determin give ## judgment mnos net sum claim respect 1.4 Analysis A term-document matrix (TDM) can now be constructed. This is a matrix listing all words in the corpus (the superset of words in all documents) and the frequency with which they occur for each document. Recall, that the processing is done on word stems. Once the processing is complete, the stem is replaced with the most prevalent unstemmed word; this is so that when word clouds are generated later, natural English words are shown rather than stems. In the chunk below, a TDM object is created and it is also transformed to a data frame for convenience. Ten random rows of the term document data frame are shown below the code chunk. # Compute term document matrix. tdm &lt;- tm::TermDocumentMatrix(tidy_corpus) tdm$dimnames$Terms &lt;- tm::stemCompletion(tdm$dimnames$Terms, before_stem_corpus) # Create term document data frame. df_tdm &lt;- tdm %&gt;% as.matrix() %&gt;% tibble::as_tibble(rownames = &#39;term&#39;) %&gt;% dplyr::filter(term != &quot;&quot;) # Output head/tail of TDM. df_tdm %&gt;% head() df_tdm %&gt;% tail() ## # A tibble: 6 x 4 ## term `Rittson-Thomas v Oxfordsh… `Kingsley v King… `Vodafone Ltd v Office … ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 aaron 0 2 0 ## 2 able 1 10 12 ## 3 absence 0 0 2 ## 4 absent 0 0 1 ## 5 absolu… 0 4 1 ## 6 abuts 0 1 0 ## # A tibble: 6 x 4 ## term `Rittson-Thomas v Oxfordsh… `Kingsley v King… `Vodafone Ltd v Office … ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 yet 0 1 4 ## 2 yieldi… 0 0 1 ## 3 young 1 0 0 ## 4 younge… 0 1 0 ## 5 zealand 0 0 1 ## 6 zone 0 1 0 1.5 Exploratory Analysis 1.5.1 Word Frequency Figure 1.1 shows the top words in each document as a bar plot. The word frequencies have been normalised by computing the term frequency: \\[ \\mathrm{TF}_{i,j} = \\frac{N_{i,j}}{\\sum_{k=1}^{K} N_{k,j}}. \\] \\(N_{i, j}\\) denotes the word count for the \\(i\\)th word in the \\(j\\)th document and \\(K\\) iterates through each word in document \\(j\\). The normalisation corrects for the fact that word frequencies would be expected to scale with the size of the document which would inflate the perceived importance of the word. # Prepare plot data. df_plt &lt;- df_tdm %&gt;% tidyr::pivot_longer(-term, names_to = &#39;document&#39;, values_to = &#39;freq&#39;) %&gt;% dplyr::group_by(document) %&gt;% dplyr::mutate(freq = freq / sum(freq)) %&gt;% dplyr::top_n(n = 10, wt = freq) %&gt;% dplyr::mutate(term = tidytext::reorder_within(term, freq, document)) # Create plot. ggplot2::ggplot(df_plt, aes(x = term, y = freq, fill = document)) + ggplot2::geom_col() + ggplot2::coord_flip() + ggplot2::labs( x = &#39;Word&#39;, y = latex2exp::TeX(&#39;Normalised frequency, $\\\\bar{f}$&#39;)) + tidytext::scale_x_reordered() + ggplot2::facet_wrap(~document, ncol = 1, scales = &#39;free_y&#39;) + ggplot2::scale_fill_discrete(guide = FALSE) + ggplot2::theme_linedraw() Figure 1.1: Bar plots of term frequencies. An alternative way to present this information is as a wordcloud, as shown in figure 1.2. Here, the word frequency is mapped to the size of the word depicted in the cloud. # Prepare plot data. set.seed(42) df_plt &lt;- df_tdm %&gt;% tidyr::pivot_longer(-term, names_to = &#39;document&#39;, values_to = &#39;freq&#39;) %&gt;% dplyr::group_by(document) %&gt;% dplyr::mutate(freq = freq / sum(freq)) %&gt;% dplyr::top_n(n = 40, wt = freq) %&gt;% mutate(angle = 90 * sample(c(0, 1), n(), replace = TRUE, prob = c(60, 40))) # Create plot. ggplot2::ggplot(df_plt, aes(label = term, size = freq, angle = angle, colour = document)) + ggwordcloud::geom_text_wordcloud(eccentricity = 1) + ggplot2::coord_equal() + ggplot2::facet_wrap(~document, ncol = 3) + ggplot2::scale_size_area(max_size = 10) + ggplot2::theme_linedraw() Figure 1.2: Word cloud for each document. These visualisations give a good idea of the keywords associated with each document. 1.5.2 Commonality Cloud Figure 1.3 shows plots a commonality cloud; this is the combined word cloud for the entire corpus where each term is normalised by the document word count. Therefore, the commonality cloud depicts words that are common across all documents in the corpus. The commonality cloud is plotted using the wordcloud package (Fellows 2018). mat_plt &lt;- as.matrix(tdm) wordcloud::commonality.cloud(mat_plt, max.words = 50) Figure 1.3: Commonality cloud. The common words identified seem reasonable; the majority relate to general vocabulary that would be expected in legal proceedings. 1.5.3 Comparison Cloud Figure 1.4 shows a comparison cloud. Contrary to the commonality cloud emphasising similarities, the comparison cloud emphasises differences in. Let \\(f_{i,j}\\) be the frequency of word \\(i\\) in document \\(j\\) and let \\(\\bar{p}_i\\) be the average frequency of word \\(i\\) in the corpus i.e. \\[ \\bar{p}_i = \\frac{1}{J}\\sum_{j=1}^{J} f_{i,j}. \\] where \\(J\\) is the number of documents in the corpus. Then, in the comparison cloud, the size of of the word is mapped to the maximum deviation i.e. \\(\\max_i(p_{i,j}-\\bar{p}_i)\\) and the position corresponds to the document in which the maximum occurs. In other words, the words shown are cases where a particular document contains many instances of the word whereas the other documents do not. The comparison cloud is plotted using the wordcloud package (Fellows 2018). mat_plt &lt;- as.matrix(tdm) # Compute colour scheme to match ggplot. gg_color_hue &lt;- function(n) { hues = seq(15, 375, length = n + 1) hcl(h = hues, l = 65, c = 100)[1:n] } colors = gg_color_hue(3) # Plot comparison cloud. colnames(mat_plt) &lt;- colnames(mat_plt) %&gt;% str_replace(&quot;v&quot;, &quot;v\\n&quot;) wordcloud::comparison.cloud(mat_plt, scale = c(6, .2), colors = colors, max.words = 50, title.size = 1) Figure 1.4: Comparison cloud. This analysis is consistent with the keywords derived from the word frequency analysis, however, keywords that are common between documents are suppressed i.e. are either not shown or are relatively smaller in size. References "],
["website-comparison-with-nlp.html", "Chapter 2 Website Comparison with NLP 2.1 Introduction 2.2 Web Scraping 2.3 Tokenisation 2.4 Term-frequency Inverse Document-Frequency 2.5 Exploratory Analysis", " Chapter 2 Website Comparison with NLP 2.1 Introduction The aim in this analysis is to: Summarise the content of websites using NLP. The analysis will consider Wikipedia pages for the purpose of demonstration and in particular will demonstrate the usefulness of the term frequency inverse document frequency (\\(\\mathrm{TFIDF}\\)) metric for NLP. 2.2 Web Scraping In this analysis, 15 Wikipedia web pages are considered. Web scraping is an activity which should be performed with courtesy. Often, a website will have a robots.txt file at its root page which sets out what is and is not acceptable. Wikipedia’s is here: https://en.wikipedia.org/robots.txt Upon inspection of this file, it is stated that: Friendly, low-speed bots are welcome viewing article pages, but not dynamically-generated pages please. This confirms that automated requests to Wikipedia are acceptable but should be maintained at a courteous speed. The Wikipedia pages that are to be scraped are as follows: # List of Wikipedia pages. BASE_URL &lt;- &quot;https://en.wikipedia.org/wiki/&quot; subject &lt;- list( # Data Science ---------------------------------------------------- `Natural Language Processing` = &quot;Natural_Language_Processing&quot;, `Data Science` = &quot;Data_science&quot;, `Computer Vision` = &quot;Computer_vision&quot;, `Artificial Intelligence` = &quot;Artificial_intelligence&quot;, `Machine Learning` = &quot;Machine_learning&quot;, # Programming Languages ------------------------------------------- `R` = &quot;R_(programming_language)&quot;, `Python` = &quot;Python_(programming_language)&quot;, `C++` = &quot;C%2B%2B&quot;, `Fortran` = &quot;Fortran&quot;, `Java` = &quot;Java_(programming_language)&quot;, # Dogs ------------------------------------------------------------ `German Shepherd` = &quot;German_Shepherd&quot;, `Spaniel` = &quot;Spaniel&quot;, `Jack Russell Terrier` = &quot;Jack_Russell_Terrier&quot;, `Staffordshire Bull Terrier` = &quot;Staffordshire_Bull_Terrier&quot;, `Dobermann` = &quot;Doberman_Pinscher&quot; ) Web pages are parsed using the xml2 package (Wickham, Hester, and Ooms 2020) and then the rvest (Wickham 2019a) package is used to extract the text content enclosed in &lt;p&gt; tags. Furthermore, child tags of the &lt;p&gt; tags with a class attribute that corresponds to maths content are removed since these do not contain ‘natural language’. # Utility function to scrape paragraph content from web pages. scrape &lt;- function(x, i) { # Courtesy delay. Sys.sleep(5) message(glue::glue(&quot;Scraping page for: {i}&quot;)) # Parse html. html_doc &lt;- xml2::read_html(paste0(BASE_URL, x)) # Get rid of nodes that contain maths. html_doc %&gt;% rvest::html_nodes(css = &quot;span[class^=&#39;mwe-math&#39;]&quot;) %&gt;% xml2::xml_remove() # Extract text from nodes. text &lt;- html_doc %&gt;% rvest::html_nodes(css = &#39;p&#39;) %&gt;% rvest::html_text() %&gt;% stringr::str_c(collapse = &quot; &quot;) %&gt;% stringr::str_squish() %&gt;% stringr::str_replace_all(&quot;([a-z])([A-Z])&quot;, &quot;\\\\1 \\\\2&quot;) # Return a data frame. tibble::tibble(doc_id = i, text = text) } # Iterate through each web page. df_html &lt;- purrr::imap_dfr(subject, scrape) An example of the text parsed for the first document is shown below. The reader may refer to the following web page to verify that the parsed text corresponds to the main web page text: https://en.wikipedia.org/wiki/Natural_Language_Processing df_html %&gt;% dplyr::slice(1) %&gt;% dplyr::pull(text) %&gt;% stringr::str_trunc(800, &#39;center&#39;, ellipsis = &#39;\\n...\\n&#39;) %&gt;% stringr::str_wrap() %&gt;% cat() Natural language processing (NLP) is a subfield of linguistics, computer science, information engineering, and artificial intelligence concerned with the interactions between computers and human (natural) languages, in particular how to program computers to process and analyze large amounts of natural language data. Challenges in natural language processing frequently involve speech recognition, ... n US patent 9269353 : Where, RMM, is the Relative Measure of Meaning token, is any block of text, sentence, phrase or word N, is the number of tokens being analyzed PMM, is the Probable Measure of Meaning based on a corpora n, is one less than the number of tokens being analyzed d, is the location of the token along the sequence of n tokens PF, is the Probability Function specific to a language 2.3 Tokenisation The content of the web pages is processed as follows: Scraped text is tokenised to single words using tidytext::unnest_tokens (Robinson and Silge 2020 ; Silge and Robinson 2016); words are automatically converted to lower-case. Words which do not solely consist of ASCII alphabetical characters are removed. Words are converted to word stems using tm::stemDocument (Feinerer and Hornik 2019). The documents are categorised as data science, programming language or dogs; this is is to assist with later visualisation and analysis. Stop words are not removed since the \\(\\mathrm{TFIDF}\\) (term-frequency inverse document-frequency) will be used to quantify the importance of a word (see 2.4); this will act to suppress or eliminate stop words. The tokenised corpus is represented by a data frame and the first rows are shown for inspection below. df_tokens &lt;- df_html %&gt;% tidytext::unnest_tokens(word, text) %&gt;% dplyr::filter(stringr::str_detect(word, &#39;^[a-z]+$&#39;)) %&gt;% dplyr::mutate(word = tm::stemDocument(word)) %&gt;% dplyr::mutate(doc_class = case_when( doc_id %in% names(subject)[1:5] ~ &quot;Data Science&quot;, doc_id %in% names(subject)[6:10] ~ &quot;Programming Language&quot;, TRUE ~ &quot;Dogs&quot; )) %&gt;% dplyr::select(doc_class, doc_id, word) df_tokens %&gt;% head() ## # A tibble: 6 x 3 ## doc_class doc_id word ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Data Science Natural Language Processing natur ## 2 Data Science Natural Language Processing languag ## 3 Data Science Natural Language Processing process ## 4 Data Science Natural Language Processing nlp ## 5 Data Science Natural Language Processing is ## 6 Data Science Natural Language Processing a 2.4 Term-frequency Inverse Document-Frequency Let \\(N_{i, j}\\) denote the word count for the \\(i\\)th word in the \\(j\\)th document. The the term frequency, \\(\\mathrm{TF}\\), for the \\(i\\)th word in the \\(j\\)th document is given by: \\[ \\mathrm{TF}_{i,j} = \\frac{N_{i,j}}{\\sum_{k=1}^{K} N_{k,j}} \\] where \\(k\\) iterates through every word in the \\(j\\)th document. The term frequency is therefore the word frequency in the document normalised with respect to the total word count for the document; in other words, it is the proportion of the document made comprised of the \\(i\\)th word. The term frequency might be considered as a measure of the importance of a word in a document; however, words that are generally important in language, such as stop words, will score highly. Similarly, if a group of documents with a common theme are to be distinguished from each other, certain words will carry less information that is specific to a document. For example, the word ‘computer’ may be common in documents relating to ‘programming languages’, hence it conveys information about the general theme but not about a specific document within that theme. The inverse document frequency, \\(\\mathrm{IDF}\\), is a metric which mitigates this: \\[ \\mathrm{IDF}_{i} = \\ln \\left[ \\frac{\\sum_{j=1}^{J}j} {\\sum_{j=1}^J \\min \\left(1, \\sum_{j=1}^{J}N_{i,j} \\right)} \\right] \\] The \\(\\mathrm{IDF}\\) is the natural logarithm of ratio of the total number of documents to the number of documents containing word \\(i\\). Therefore, if word \\(i\\) occurs in all documents, the \\(\\mathrm{IDF}\\) is 0 and the \\(\\mathrm{IDF}\\) increases as the word rarity word increases. Note that the logarithmic relationship is derived from empirical studies of word importance ranking relative to natural language. The product of \\(\\mathrm{TF}\\) and \\(\\mathrm{IDF}\\) is a common metric to measure the importance of a word. The \\(\\mathrm{IDF}\\) acts to diminish the \\(\\mathrm{TF}\\) metric for words which are generally common in the corpus. Hence, the term-frequency inverse document-frequency is often a useful metric for ranking distinctive words in a document. The \\(\\mathrm{TFIDF}\\) metric is calculated using tidytext::bind_tf_idf (Robinson and Silge 2020). df_tf_idf &lt;- df_tokens %&gt;% dplyr::group_by(doc_class, doc_id) %&gt;% dplyr::count(word, sort = TRUE) %&gt;% dplyr::ungroup() %&gt;% tidytext::bind_tf_idf(word, doc_id, n) Below, the metrics for the words with the lowest and highest \\(\\mathrm{TFIDF}\\) are shown. The word count ‘the’ is very common in each document i.e. \\(n\\) is large. However, the \\(\\mathrm{IDF}\\) is 0 since it occurs in all documents, thus \\(\\mathrm{TFIDF}\\) is also 0. On the contrary, the word ‘dobermann’ has relatively few counts in the document on Dobermanns but since it does not occur in other documents, it has a high \\(\\mathrm{IDF}\\) i.e. \\(\\mathrm{IDF} = ln(15/1) = 2.71\\). ## Lowest TFIDF... ## # A tibble: 6 x 7 ## doc_class doc_id word n tf idf tf_idf ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Data Science Artificial Intelligence the 679 0.0491 0 0 ## 2 Data Science Artificial Intelligence of 461 0.0334 0 0 ## 3 Data Science Artificial Intelligence to 431 0.0312 0 0 ## 4 Data Science Artificial Intelligence and 406 0.0294 0 0 ## 5 Dogs Jack Russell Terrier the 330 0.0821 0 0 ## 6 Programming Language Fortran the 326 0.0605 0 0 ## Highest TFIDF... ## # A tibble: 6 x 7 ## doc_class doc_id word n tf idf tf_idf ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Programming Language Python python 141 0.0390 1.32 0.0515 ## 2 Programming Language Java java 163 0.0411 1.32 0.0543 ## 3 Dogs Dobermann dobermann 56 0.0201 2.71 0.0545 ## 4 Programming Language Fortran fortran 183 0.0340 2.01 0.0684 ## 5 Dogs Spaniel spaniel 37 0.0373 2.01 0.0751 ## 6 Programming Language R r 107 0.0523 1.61 0.0842 2.5 Exploratory Analysis The word clouds for the \\(\\mathrm{TFIDF}\\) of each document are shown in figure 2.1. The word cloud data source is a corpus which does not explicitly convey the title of the document. Notice how the highest scoring words based on \\(\\mathrm{TFIDF}\\) enable a good estimate of the likely title words. # Utility function to plot word cloud. plot_word_cloud &lt;- function(df, max_size) { # Prepare data. df_plt &lt;- df %&gt;% dplyr::group_by(doc_class, doc_id) %&gt;% dplyr::top_n(30, wt = tf_idf) %&gt;% dplyr::mutate(angle = 90 * sample(c(0, 1), n(), replace = TRUE, prob = c(60, 40))) # Create plot ggplot(df_plt, aes(label = word, size = tf_idf, angle = angle)) + ggwordcloud::geom_text_wordcloud(seed = 42, eccentricity = 1, shape = &#39;square&#39;) + ggplot2::scale_size_area(max_size = max_size) + ggplot2::coord_equal() + ggplot2::facet_wrap(~doc_class + doc_id, dir = &#39;v&#39;, ncol = 3) + ggplot2::theme_linedraw() } # Plot word cloud. plot_word_cloud(df_tf_idf, max_size = 14) Figure 2.1: Word clouds faceted by document categories. It is also insightful to remove the title words and recalculate the \\(\\mathrm{TFIDF}\\) word cloud. The title words tend to overwhelm the other distinctive i.e. information-carrying words. The resulting word cloud is shown in figure 2.2. # Utility function to remove stemmed title words. eliminate_words &lt;- function(title) { stringr::str_split(title, &quot; &quot;)[[1]] %&gt;% tm::stemDocument() %&gt;% stringr::str_to_lower() } df_tf_idf_no_tw &lt;- df_tokens %&gt;% dplyr::group_by(doc_id) %&gt;% # Eliminate title words. dplyr::filter(!(word %in% eliminate_words(doc_id))) %&gt;% # Remove acronyms and misspellings. dplyr::filter(!(doc_id == &#39;C++&#39; &amp; word == &#39;c&#39;)) %&gt;% dplyr::filter(!(doc_id == &#39;Dobermann&#39; &amp; word == &#39;doberman&#39;)) %&gt;% dplyr::filter( !(doc_id == &#39;Staffordshire Bull Terrier&#39; &amp; word == &#39;stafford&#39;)) %&gt;% dplyr::filter(!(doc_id == &#39;Artificial Intelligence&#39; &amp; word == &#39;ai&#39;)) %&gt;% dplyr::ungroup() %&gt;% # Compute TFIDF dplyr::group_by(doc_class, doc_id) %&gt;% dplyr::count(word, sort = TRUE) %&gt;% dplyr::ungroup() %&gt;% tidytext::bind_tf_idf(word, doc_id, n) # Plot word cloud plot_word_cloud(df_tf_idf_no_tw, max_size = 10) Figure 2.2: Word clouds faceted by document categories with title document title words removed from the corpus. From figure 2.2 the high scoring words seem consistent with experience in these subjects and this confirms that the \\(\\mathrm{TFIDF}\\) metric is a useful indicator of the words that best describe what is distinctive about the contents of a document. Notice that the word ‘dog’ is significant for each of the 5 pages relating to dogs. This is to be expected since the word ‘dog’ is not a generally significant word for the 15 document corpus but is significant for these 5 articles. Therefore, the \\(\\mathrm{TFIDF}\\) is relatively large. To demonstrate the usefulness of the \\(\\textrm{TFIDF}\\) metric, it will be recalculated for only the dog articles. The resulting word cloud is shown in figure @ref{fig:plt-wordcloud-dogs}. It is observed that the word ‘dog’ is eliminated since although the absolute count of the word for each document has not changed, it is now common to all documents, hence the \\(\\mathrm{IDF}\\) is 0. df_tf_idf_dogs &lt;- df_tokens %&gt;% dplyr::filter(doc_class == &#39;Dogs&#39;) %&gt;% dplyr::group_by(doc_class, doc_id) %&gt;% dplyr::count(word, sort = TRUE) %&gt;% dplyr::filter(!(word %in% eliminate_words(doc_id))) %&gt;% dplyr::filter(!(doc_id == &#39;Dobermann&#39; &amp; word == &#39;doberman&#39;)) %&gt;% dplyr::filter( !(doc_id == &#39;Staffordshire Bull Terrier&#39; &amp; word == &#39;stafford&#39;)) %&gt;% dplyr::ungroup() %&gt;% tidytext::bind_tf_idf(word, doc_id, n) plot_word_cloud(df_tf_idf_dogs, max_size = 10) Figure 2.3: Word clouds for documents relating to dogs. References "],
["ch-advanced-pdf-parsing.html", "Chapter 3 Advanced PDF Parsing 3.1 Introduction 3.2 Method 3.3 Conclusion", " Chapter 3 Advanced PDF Parsing 3.1 Introduction 3.1.1 Objective This report addresses the objective: Given a large PDF document with several chapters/sections, apply NLP (Natural Language Processing) techniques to each section in order to gain insight into the content of each section. The document being considered for this example is Shakespeare’s Macbeth and the objective is to split the document into a corpus of sections. 3.1.2 HTML Documents Parsing an HTML (Hyper-text Markup Language) document i.e. a web page by section is relatively easy since the HTML markup language explicitly encodes the structure of the document i.e. HTML directly specifies whether each item of text is a heading, paragraph, list, etc. Headings are typically enclosed by one of six HTML markup tags i.e. &lt;h1&gt;, &lt;h2&gt;, etc. wherein the number succeeding the ‘h’ reflects the hierarchical importance of the heading. For example, &lt;h1&gt; tags may reflect a ‘title’ heading, &lt;h1&gt; a ‘chapter’, &lt;h2&gt; a section and so forth. HTML only describes the structure of the document; the formatting of the document, describing how the text is rendered is (usually) described elsewhere e.g. a CSS (Cascading Style Sheet) file. 3.1.3 PDF Documents The raw encoding for a PDF document is fundamentally different to that of an HTML document. Whereas the raw code for HTML contains the document content and explicit encoding of the document structure, a PDF document describes the content and the formatting e.g. font style, font size, character location, etc. The structure is therefore implicit, rather than explicit and it is only possible to infer the structural aspects of the document from the code describing the formatting of the document. For example, if an item of text is formatted to be rendered with a large font, it could be inferred that the text is a ‘title’ heading. In this report, a method for parsing a PDF document will be described. The method is facilitated by the pdftools package (Ooms 2020)1. 3.2 Method 3.2.1 Read Document Using pdftools::pdf_data the document is read, word-by-word. For later convenience, the document line number of each word is computed and included with the data. The first and last words of extracted raw data are inspected: # Download document to temporary location. destfile = tempfile(&#39;macbeth&#39;, fileext = &#39;.pdf&#39;) download.file(url = &#39;https://www.williamshakespeare.net/pdf/macbeth.pdf&#39;, destfile = destfile, quiet = TRUE) # Read metadata. df_doc_data_raw &lt;- pdftools::pdf_data(destfile) %&gt;% dplyr::bind_rows(.id = &#39;page&#39;) %&gt;% dplyr::mutate(page = as.integer(page)) %&gt;% dplyr::mutate(text = text %&gt;% stringr::str_remove_all(&#39;[^[:print:]]+&#39;) %&gt;% stringr::str_squish()) # Remove temporary file. unlink(destfile) # Compute line numbers. line_no &lt;- df_doc_data_raw %&gt;% dplyr::distinct(page, y) %&gt;% dplyr::arrange(page, y) %&gt;% dplyr::mutate(doc_line = row_number()) df_doc_data_raw %&lt;&gt;% dplyr::left_join(line_no, by = c(&#39;page&#39;, &#39;y&#39;)) # Print. df_doc_data_raw %&gt;% head() df_doc_data_raw %&gt;% tail() ## # A tibble: 6 x 8 ## page width height x y space text doc_line ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;lgl&gt; &lt;chr&gt; &lt;int&gt; ## 1 1 48 11 458 38 TRUE HAMLET 1 ## 2 1 3 11 510 38 TRUE - 1 ## 3 1 17 11 517 38 TRUE Act 1 ## 4 1 8 11 538 38 FALSE V 1 ## 5 1 6 11 535 794 FALSE 1 2 ## 6 2 58 15 54 81 FALSE Contents 3 ## # A tibble: 6 x 8 ## page width height x y space text doc_line ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;lgl&gt; &lt;chr&gt; &lt;int&gt; ## 1 83 42 12 197 476 TRUE crown&#39;d 3360 ## 2 83 9 12 244 476 TRUE at 3360 ## 3 83 36 12 257 476 FALSE Scone. 3360 ## 4 83 50 12 54 505 TRUE Flourish. 3361 ## 5 83 37 12 108 505 FALSE Exeunt 3361 ## 6 83 13 11 529 794 FALSE 83 3362 The extracted metadata is: text: the word page: the page that the word occurs on width: the width of the word i.e. font width height: the height of the word i.e. font height x: the horizontal location of the word on the page y: the vertical location of the word on the page space: whether the word should have a space appended The line number, doc_line, is computed from the raw metadata by identifying unique combinations of page and y. As stated above, the word metadata demonstrates that the internal representation of a PDF combines the description of the formatting and the content and does not explicitly describe the structure. 3.2.2 Identify Headings It may be possible to extract further metadata relating to each word such as font type; however, this has not been explored presently. For the method described herein, the word height is used to identify candidate words that may be section heading content. Some exploration indicated that words that are of height 22 are likely to be section headings for the document being analysed: df_doc_data_raw %&gt;% dplyr::filter(height == HEADING_SIZE) ## # A tibble: 10 x 8 ## page width height x y space text doc_line ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;lgl&gt; &lt;chr&gt; &lt;int&gt; ## 1 3 47 22 54 56 TRUE ACT 38 ## 2 3 6 22 108 56 FALSE I 38 ## 3 20 47 22 54 116 TRUE ACT 751 ## 4 20 13 22 108 116 FALSE II 751 ## 5 34 47 22 54 239 TRUE ACT 1322 ## 6 34 20 22 108 239 FALSE III 1322 ## 7 51 47 22 54 71 TRUE ACT 2019 ## 8 51 22 22 108 71 FALSE IV 2019 ## 9 69 47 22 54 229 TRUE ACT 2779 ## 10 69 16 22 108 229 FALSE V 2779 The raw data frame contains a row for each word. The objective of the following code is to traverse the data frame above and derive groups of words that form each individual section heading. This is achieved by first filtering words that have the section heading height. Subsequently, by joining these filtered words that either occur on the same line or on consecutive lines, section headings can be inferred. It is necessary to consider consecutive lines since it is possible that long section headings contain a line break although this does not occur in the current document. The procedure in the code block below is: Filter words with a height matching the section heading height. Identify whether the words in the next row(s) are also part of the same heading i.e. on the same or consecutive lines in the document. Assign a unique ID to words belonging to the same heading. Concatenate words belonging to the same heading. # Derive headings. df_headings &lt;- df_doc_data_raw %&gt;% # Filter words that make up headings. dplyr::filter(height == HEADING_SIZE) %&gt;% # Group words that form each heading and assign head_ID. dplyr::mutate(doc_line_diff = doc_line - lag(doc_line)) %&gt;% tidyr::replace_na(list(doc_line_diff = -1)) %&gt;% dplyr::mutate(head_id = cumsum(doc_line_diff &gt; 1)) %&gt;% # Concatenate words with the same head_ID. dplyr::group_by(head_id) %&gt;% dplyr::summarise(doc_line = first(doc_line), page = first(page), heading = str_c(text, collapse = &quot; &quot;), .groups = &#39;drop&#39;) df_headings ## # A tibble: 5 x 4 ## head_id doc_line page heading ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; ## 1 0 38 3 ACT I ## 2 1 751 20 ACT II ## 3 2 1322 34 ACT III ## 4 3 2019 51 ACT IV ## 5 4 2779 69 ACT V The reader may refer to the imported document, to confirm that the section headings are correctly captured here. The section heading words can now be joined to the raw data. The procedure in the following code chunk is: Select the required metadata. Join the section headings derived above to the first word of each section in the raw data. Fill the section headings forwards so that the text between heading A and heading B is assigned to heading A. Note that the last page (of the last section) should be assigned otherwise the last the section will be assumed to continue until the end of the text. # Assign headings to raw data. df_doc_data &lt;- df_doc_data_raw %&gt;% dplyr::select(page, doc_line, text) %&gt;% dplyr::filter(page &lt;= LAST_PAGE) %&gt;% # Merge headings. dplyr::left_join(df_headings, by = c(&#39;page&#39;, &#39;doc_line&#39;)) %&gt;% # Fill headings forward tidyr::fill(heading, head_id) %&gt;% tidyr::drop_na() 3.2.3 Corpus by Heading Finally, the data frame is grouped by section and the words within the section are concatenated to form the document corpus. df_corpus &lt;- df_doc_data %&gt;% dplyr::group_by(heading) %&gt;% summarise(corpus = text %&gt;% str_c(collapse = &quot; &quot;) %&gt;% stringr::str_trunc(500, &#39;center&#39;) %&gt;% stringr::str_wrap(), .groups = &#39;drop&#39;) cat(df_corpus$corpus, sep = &#39;\\n--------\\n&#39;) ## ACT I SCENE I. A desert place. Thunder and lightning. Enter three Witches First ## Witch When shall we three meet again In thunder, lightning, or in rain? Second ## Witch When the hurlyburly&#39;s done, When the battle&#39;s lost and won. Third Witch ## That will be...her, As we shall make our griefs and clamour roar Upon his death? ## MACBETH I am settled, and bend up Each corporal agent to this terrible feat. 19 ## Away, and mock the time with fairest show: False face must hide what the false ## heart doth know. Exeunt ## -------- ## ACT II SCENE I. Court of Macbeth&#39;s castle. Enter BANQUO, and FLEANCE bearing ## a torch before him BANQUO How goes the night, boy? FLEANCE The moon is down; I ## have not heard the clock. BANQUO And she goes down at twelve. FLEANCE I take&#39;t, ## &#39;tis later, s..., I will thither. MACDUFF Well, may you see things well done ## there: adieu! Lest our old robes sit easier than our new! ROSS Farewell, father. ## Old Man God&#39;s benison go with you; and with those That would make good of bad, ## and friends of foes! Exeunt ## -------- ## ACT III SCENE I. Forres. The palace. Enter BANQUO BANQUO Thou hast it now: king, ## Cawdor, Glamis, all, As the weird women promised, and, I fear, Thou play&#39;dst ## most foully for&#39;t: yet it was said It should not stand in thy posterity, But ## that myself sh...istance His wisdom can provide. Some holy angel Fly to the ## court of England and unfold His message ere he come, that a swift blessing May ## soon return to this our suffering country Under a hand accursed! Lord I&#39;ll send ## my prayers with him. 50 Exeunt ## -------- ## ACT IV SCENE I. A cavern. In the middle, a boiling cauldron. Thunder. Enter the ## three Witches First Witch Thrice the brinded cat hath mew&#39;d. Second Witch Thrice ## and once the hedge-pig whined. Third Witch Harpier cries &#39;Tis time, &#39;tis time. ## First Wit...e goes manly. Come, go we to the king; our power is ready; Our lack ## is nothing but our leave; Macbeth Is ripe for shaking, and the powers above Put ## on their instruments. Receive what cheer you may: The night is long that never ## finds the day. Exeunt ## -------- ## ACT V SCENE I. Dunsinane. Ante-room in the castle. Enter a Doctor of Physic ## and a Waiting-Gentlewoman Doctor I have two nights watched with you, but can ## perceive no truth in your report. When was it she last walked? Gentlewoman ## Since his majesty wen...nt hands Took off her life; this, and what needful else ## That calls upon us, by the grace of Grace, We will perform in measure, time ## and place: So, thanks to all at once and to each one, Whom we invite to see us ## crown&#39;d at Scone. Flourish. Exeunt 83 The data is now in a form whereby NLP (Natural Language Processing) methods can by applied by section. 3.3 Conclusion It is possible to infer PDF section headings by manipulating the metadata available from the PDF encoding. It is considerably more challenging to traverse the section hierarchy of PDF documents than HTML documents. The method described herein requires assumptions and interpretations in order to infer what text is a section heading and what the hierarchy is. In contrast, markup tags make this explicit in HTML documents. For section-by-section processing of a PDF document, human inspection and interpretation would be required for each document of study i.e. to determine likely section heading heights. References "],
["references.html", "References", " References "]
]
